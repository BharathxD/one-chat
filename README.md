# OneChat

![OG Image](https://www.1chat.tech/opengraph-image.jpg)

> Open-source AI chat application that brings together the best of multiple AI models in one seamless experience.

## What is OneChat?

OneChat is a modern AI chat platform that lets you access multiple AI models from different providers in a single interface. Instead of juggling multiple subscriptions and interfaces, you can compare responses from GPT-4o/4.1, Claude, Gemini, and other models.

This version features a Next.js frontend and a backend API built with Rust (Axum) and MongoDB.

### Key Benefits

- **Multiple AI Models**: Access OpenAI, Anthropic, Google, and open-source models in one place via the Axum backend.
- **Real-Time Streaming**: See AI responses as they're generated.
- **Cross-Device Sync**: Your conversations sync between all your devices (managed by the backend).
- **Secure API Key Management**: Supports Bring-Your-Own-Key (BYOK) model; keys are handled by the backend.
- **Advanced Features**: Voice input/output, file attachments (via Vercel Blob), web search, and chat sharing.

## Features

### AI Model Support
The Axum backend is designed to support various AI providers:
- **OpenAI**: GPT-4, GPT-4 Turbo, GPT-3.5
- **Anthropic**: Claude 3.5 Sonnet, Claude 3 Opus, Claude 3 Haiku
- **Google**: Gemini Pro, Gemini Ultra
- **Open Source Models**: Llama 3, Qwen, DeepSeek (e.g., through OpenRouter)
- **OpenRouter**: Access a wide array of models.
- The backend includes an OpenAI-compatible endpoint, and aims to support Ollama and LMStudio compatible interfaces.

### Real-Time Features
- Stream AI responses in real-time from the Axum API.
- Share conversations with live updates.
- Resumable streams (if supported by backend implementation).

### Voice & Speech
- Text-to-speech using OpenAI and Google models (via Axum API).
- Real-time speech-to-text transcription with GPT-4o mini (client token generated by Axum API).

### File Support & Media
- Upload multiple files (attachment deletion managed via Axum API and Vercel Blob).
- Syntax highlighting for code and math formulas (frontend feature).

### Web Search
- (If implemented) Native Search or FireCrawl integration via the backend.

### Chat Management
- **Branching**: Try the same prompt with different models.
- **Selective Sharing**: Share full conversations or just up to a specific message.
- **Personalization**: Configure AI personality and response preferences (managed by frontend, settings potentially stored via backend).

## Technical Stack

### Frontend (`apps/web`)
- **React 19** with **Next.js 15**
- **TypeScript** for type safety
- **TailwindCSS 4** for styling
- **TanStack Query (React Query)** for server state management, interacting with the Axum API.

### Backend (`api/`)
- **Rust** with **Axum** web framework.
- **MongoDB** for data storage.
- **Redis** (Upstash recommended for cloud, local for dev) for rate limiting and potential caching.
- Direct integration with AI services (OpenAI, Google, OpenRouter, etc.) using `reqwest`.

### Infrastructure
- **TurboRepo** for monorepo management.
- Frontend deployed on **Vercel** (recommended).
- Backend API containerized with **Docker**, deployable to various container services (Fly.io, Render, Cloud Run, ECS, etc.).

## Getting Started

### Prerequisites
- Node.js 18+ and pnpm
- Docker and Docker Compose (for backend services)
- Access to MongoDB and Redis instances (local setup via included Docker Compose, or cloud instances).

### Installation & Setup

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/your-username/one-chat.git # Replace with actual repo URL
    cd one-chat
    ```

2.  **Backend API Setup (`api/`):**
    *   The backend consists of the Axum API, MongoDB, and Redis.
    *   Copy `api/.env.example` to `api/.env` and fill in necessary values (especially `JWT_SECRET`, `OPENAI_API_KEY` if using server-side OpenAI, `VERCEL_BLOB_READ_WRITE_TOKEN` if using attachments).
        *   `DATABASE_URL` for MongoDB and `REDIS_URL` will be used by the Docker Compose setup if running locally.
    *   To run the backend services locally (API, MongoDB, Redis):
        ```bash
        docker-compose up --build
        ```
        (This `docker-compose.yml` is at the project root.)
        The API will typically be available at `http://localhost:3001`.

3.  **Frontend Setup (`apps/web/`):**
    *   Install frontend dependencies:
        ```bash
        pnpm install
        ```
    *   Navigate to the web app directory:
        ```bash
        cd apps/web
        ```
    *   Create a local environment file by copying the example:
        ```bash
        cp .env.example .env.local
        ```
    *   Edit `.env.local` and ensure `NEXT_PUBLIC_APP_URL` is correct (e.g., `http://localhost:3000` for local dev).
    *   **Crucially, set `NEXT_PUBLIC_AXUM_API_URL`** to point to your running Axum API (e.g., `http://localhost:3001/api` if running backend locally as per Docker Compose).
    *   Fill in other necessary variables like `GOOGLE_CLIENT_ID`, `GOOGLE_CLIENT_SECRET` for `better-auth`.

4.  **Start Frontend Development Server:**
    From the project root:
    ```bash
    pnpm --filter web dev
    ```
    Or from the `apps/web` directory:
    ```bash
    pnpm dev
    ```
    Open `http://localhost:3000` in your browser.

### API Key Setup (Bring Your Own Key - BYOK)

- The application supports using your own API keys for various AI services.
- These keys are typically entered in the frontend application's settings.
- The frontend then sends these keys (or the relevant one for a chosen model/provider) to the Axum API with each request.
- The Axum API uses the provided key to interact with the third-party AI service.
- Server-side fallback keys (e.g., for OpenAI in `api/.env`) might be used for some features like title generation or if a user hasn't provided their own key (and if rate limiting is in place for such use).

## API Documentation

For details on the Axum API endpoints, please refer to [API_DOCUMENTATION.md](API_DOCUMENTATION.md).

## Usage

1. Sign in with Google on the frontend.
2. (Optional but Recommended) Add your API keys for LLM providers in the frontend settings.
3. Start a new conversation and select your preferred model/provider.
4. Try out voice input, file uploads, or web search.

### Tips
- Switch models mid-conversation to compare responses.
- Use chat branching to explore different approaches to the same question.
- Share conversations with colleagues.

## Contributing

We welcome contributions!
1. Fork the repo.
2. Create a feature branch: `git checkout -b feature/your-feature`.
3. Make your changes and test them.
4. Submit a pull request.

### Development Guidelines
- Use TypeScript for the frontend, Rust for the backend.
- Follow the existing code style.

## License

MIT License - see [LICENSE](LICENSE) for details.

## Acknowledgments

Inspired by the need for a unified AI chat experience. Built with Next.js, Axum (Rust), MongoDB, and leveraging powerful models from OpenAI, Anthropic, Google, and the open-source community.
---

Questions? Issues? Feel free to open an issue or start a discussion.
